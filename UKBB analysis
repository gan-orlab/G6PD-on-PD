UKBB analysis

### Step 1 do tabix for UKBB files
module load nixpkgs/16.09
module load intel/2018.3
module load tabix/0.2.6

tabix -f -p vcf ukb23156_cX_b23_v1.vcf.gz


#step 2 , do GATK with GQ30 DP20 also remove multiallelic sites
## To get GQ30 DP20 from ukb23156_cX_b23_v1.vcf.gz use bcf tools 
bcftools filter -i 'GQ >= 20 & DP >= 30' ukb23156_cX_b23_v1.vcf.gz -Oz -o UKBB_GQ_DP.vcf.gz

### bcftools view -i 'F_MISSING<0.05' UKBB_GQ_DP.vcf.gz -Oz -o UKBB_GQ_DP_missing_filtered.vcf.gz

## Then do GATK with GQ30 DP20 also remove multiallelic sites using this script 
 vcftools --gzvcf UKBB_GQ_DP.vcf.gz --max-alleles 2 --min-alleles 2 --recode --stdout | gzip -c > UKBB_GQ20_DP30_MISS_filtered_2alleles_vcf.gz

### convert to b-files
plink --vcf UKBB_GQ20_DP10_MISS_filtered_2alleles_vcf.gz   --vcf-half-call m --make-bed --out UKBB_GQ20_DP10_MISS_filtered_2alleles_vcf.gz_after_GATK

###
plink2 --bfile UKBB_GQ20_DP10_MISS_filtered_2alleles_vcf.gz_after_GATK --out all_rare_ukbb --export vcf bgz id-paste=fid

### OR extract your specified base pair gene already 
 plink --bfile UKBB_GQ20_DP10_MISS_filtered_2alleles_vcf.gz_after_GATK --chr X --from-bp 154531391 --to-bp 154546846 --make-bed --out UKBB_G6PD_part


### convert vcf to ANNOVAR format
 perl ~/runs/leahc/AMP_G6PD/annovar/convert2annovar.pl --format vcf4 UKBB_GQ20_DP10_MISS_filtered_2alleles_vcf.gz --allsample --withfreq --outfile UKBB_recode_convert   ## OR USE ##  perl ~/runs/leahc/AMP_G6PD/annovar/convert2annovar.pl --format vcf4 UKBB_GLA_part.vcf.gz --allsample --withfreq --outfile UKBB_GLA_recode_convert


### annotate all snps
perl ~/runs/leahc/AMP_G6PD/annovar/table_annovar.pl UKBB_recode_convert ~/runs/leahc/AMP/annovar/humandb/ --buildver hg38 --out UKBB_recode_conver_annovar --remove --protocol refGene,ljb26_all,dbnsfp41c --operation g,f,f --nastring .  (THIS SCRIPT HAS ISSUES BECAUSE OF TWO f,f rather take the one from UKBB)

### CADD Variants
awk -F '\t' 'NR>1 && $31>20 {print $1" "$2" "$3" "$7}' UKBB_recode_conver_annovar.hg38_multianno.txt > UKBB_CADD.txt

### NS
awk -F '\t' '$9 ~ /nonsyn/ {print $1" "$2" "$3" "$7}' UKBB_recode_conver_annovar.hg38_multianno.txt > UKBB_nonsyn.txt


### SKAT-O analysis
#Option 2
## Load libraries
install.packages("dplyr")
install.packages("SKAT")
install.packages("bigsnpr")

## load
library(dplyr)
library(SKAT)
library(bigsnpr)

# Memory management
options(bigsnpr.download = FALSE)
gc()

## Set paths
setwd("~/Desktop/R_things/SKAT")
snp_readBed("UKBB_GQ20_DP30_MAF_0.01_covar_mac2.bed") 

bfile <- "~/Desktop/R_things/SKAT/UKBB_GQ20_DP30_MAF_0.01_covar_mac2"
covar_file <- "covar_UKBB_prxy_females_binary_1to10.txt"
setid_file <- "UKBB_wproxies_G6PD_SETID.txt"

## Load data
genotype_data <- snp_attach(paste0(bfile, ".rds"))
covariates <- read.table(covar_file, header = TRUE) %>%
mutate(Sex = ifelse(as.numeric(Sex) == 0, NA, as.numeric(Sex)))

# Match samples
matching_indices <- match(covariates$IID, genotype_data$fam$sample.ID)
covariates <- covariates[!is.na(matching_indices), ]
matching_indices <- na.omit(matching_indices)
# Store sample count
nSamples <- length(matching_indices)


## Create null model
obj.x <- SKAT_Null_Model_ChrX(Status ~ Age + Sex, SexVar = "Sex", 
                             out_type = "D", data = covariates)

## Load SETID data
setid_data <- read.table(setid_file, header = TRUE, sep = "\t")
setid_list <- split(setid_data$Variant_ID, setid_data$SET_ID_name)
snp_ids <- genotype_data$map$marker.ID

## Ultra-conservative chunked processing (20 SNPs at a time)
run_skat_super_chunked <- function(snp_indices, chunk_size = 20) {
  n_snps <- length(snp_indices)
  n_chunks <- ceiling(n_snps/chunk_size)
  
  # Initialize results
  all_results <- list(
    SKAT = numeric(n_chunks),
    Burden = numeric(n_chunks),
    SKATO = numeric(n_chunks)
  )
  
  for (i in 1:n_chunks) {
    chunk_start <- (i-1)*chunk_size + 1
    chunk_end <- min(i*chunk_size, n_snps)
    current_indices <- snp_indices[chunk_start:chunk_end]
    
    # Process this small chunk
    Z_chunk <- genotype_data$genotypes[matching_indices, current_indices, drop = FALSE]
    
    all_results$SKAT[i] <- SKAT_ChrX(Z_chunk, obj.x, kernel = "linear.weighted", 
                                    is_X.inact = FALSE)$p.value
    all_results$Burden[i] <- SKAT_ChrX(Z_chunk, obj.x, kernel = "linear.weighted", 
                                      r.corr = 1, is_X.inact = FALSE)$p.value
    all_results$SKATO[i] <- SKAT_ChrX(Z_chunk, obj.x, kernel = "linear.weighted", 
                                     method = "SKATO", is_X.inact = FALSE)$p.value
    
    
    # Aggressive memory cleanup
    rm(Z_chunk)
    gc()
    
    # Progress report
    if (i %% 5 == 0) cat("Processed chunk", i, "of", n_chunks, "\n")
  }
  
   #Fisher's method to combine p-values
   combine_p <- function(pvals) {
    pvals <- pvals[!is.na(pvals)]
    if (length(pvals) == 0) return(NA)
    pchisq(-2 * sum(log(pvals)), df = 2*length(pvals), lower.tail = FALSE)
  }
  
  
  list(
    SKAT = combine_p(all_results$SKAT),
    Burden = combine_p(all_results$Burden),
    SKATO = combine_p(all_results$SKATO)
  )
}

## Process each gene set
results <- list()

for (set in names(setid_list)) {
  cat("\nProcessing", set, "...\n")
  
  
  snps_in_set <- setid_list[[set]]
  snp_indices <- which(snp_ids %in% snps_in_set)
  
  
  if (length(snp_indices) == 0) {
    warning(paste("No SNPs found for", set))
    next
  }
  
  # Use ultra-conservative chunking for large sets
  if (length(snp_indices) > 20) {
    cat("Large set detected (", length(snp_indices), "SNPs), using chunked processing...\n")
    results[[set]] <- run_skat_super_chunked(snp_indices)
  } else {
    cat("Small set (", length(snp_indices), "SNPs), processing normally...\n")
    Z_subset <- genotype_data$genotypes[matching_indices, snp_indices, drop = FALSE]
    results[[set]] <- list(
      SKAT = SKAT_ChrX(Z_subset, obj.x, kernel = "linear.weighted", is_X.inact = FALSE)$p.value,
      Burden = SKAT_ChrX(Z_subset, obj.x, kernel = "linear.weighted", r.corr = 1, is_X.inact = FALSE)$p.value,
      SKATO = SKAT_ChrX(Z_subset, obj.x, kernel = "linear.weighted", method = "SKATO", is_X.inact = FALSE)$p.value
    )
  }
  
  
  # Save intermediate results every 5 sets
  if (length(results) %% 5 == 0) {
    temp_file <- paste0("SKAT_temp_results_", length(results), ".rds")
    saveRDS(results, temp_file)
    cat("Saved temporary results to", temp_file, "\n")
  }
}

## Create final results
results_df <- data.frame(
  SetID = names(results),
  SKAT_p = sapply(results, function(x) ifelse(is.null(x$SKAT), NA, x$SKAT)),
  Burden_p = sapply(results, function(x) ifelse(is.null(x$Burden), NA, x$Burden)),
  SKATO_p = sapply(results, function(x) ifelse(is.null(x$SKATO), NA, x$SKATO)),
  nSNPs = sapply(setid_list[names(results)], length),
  stringsAsFactors = FALSE
)

## Print results to console with formatting
cat("\n\n=== SKAT ANALYSIS RESULTS ===\n\n")

# Print all results if small, or just top/bottom if large
#if (nrow(results_df) <= 20) {
 # print(results_df)
#} else {
  #cat("First 10 results:\n")
  #print(head(results_df, 10))
  #cat("\nLast 10 results:\n")
  #print(tail(results_df, 10))
 # cat("\n(Showing 20 of", nrow(results_df), "total gene sets)\n")
#}


# Add nSamples column (same for all rows, from matching_indices length)
results_df$nSamples <- length(matching_indices)


# Print all results if small, or just top/bottom if large
if (nrow(results_df) <= 20) {
  print(results_df)
} else {
  cat("First 10 results:\n")
  print(head(results_df, 10))
  cat("\nLast 10 results:\n")
  print(tail(results_df, 10))
  cat("\n(Showing 20 of", nrow(results_df), "total gene sets; each with",
      unique(results_df$nSamples), "samples)\n")
}


# Add summary statistics
cat("\n=== SUMMARY STATISTICS ===\n")
cat("Number of gene sets analyzed:", nrow(results_df), "\n")
cat("Range of p-values:\n")
cat("  SKAT:   ", format(range(results_df$SKAT_p, na.rm = TRUE), digits = 3), "\n")
cat("  Burden: ", format(range(results_df$Burden_p, na.rm = TRUE), digits = 3), "\n")
cat("  SKATO:  ", format(range(results_df$SKATO_p, na.rm = TRUE), digits = 3), "\n")


### LOF
awk -F '\t' '$9 ~ /stop|frame/ || $6 ~ /splicing/ {print $1" "$2" "$3" "$7}'   UKBB_recode_conver_annovar.hg38_multianno.txt  > UKBB_LOF.txt

### Functional
awk -F '\t' '$9 ~ /nonsyn|stop|frame/ || $6 ~ /splicing/ {print $1, $2, $3, $7}'  UKBB_recode_conver_annovar.hg38_multianno.txt  > UKBB_functional.txt

### To prepare for RV tests
## Extract variants from annotated for UKBB_CADD,LOF,nonsyn and functional or it means combine and match CADD etc and UKBB bfiles
## Do the rest for nonsyn, LOF and functional 
plink2 --bfile UKBB_GQ20_DP10_MISS_filtered_2alleles_vcf.gz_after_GATK --extract range UKBB_CADD.txt --export vcf bgz id-paste=fid --out UKBB_CADD_final

### Then run tabix again using the recent (up) UKBB_CADD_final ... etc (other)
tabix -f -p vcf UKBB_CADD_final.vcf.gz

### Then to extract G6PD from UKBB_CADD_final.vcf.gz use bcftools?
bcftools view -i 'ID="G6PD"' -Oz -o UKBB_G6PD_nonsyn.vcf.gz UKBB_nonsyn_final.vcf.gz

### to get G6PD.txt hg38 format
gunzip refFlat.txt.gz
grep G6PD refFlat.txt > G6PD.txt


### Run RV
 rvtest --inVcf UKBB_G6PD_functional_final.vcf.gz --burden cmcWald--kernel skato --geneFile G6PD.txt  --covar covar_WITHproxy_UKBB.txt --covar-name Age,Sex --out UKBB_WITHproxy_G6PD_functional --numThread 4 --freqUpper 0.01 --pheno covar_WITHproxy_UKBB.txt --pheno-name PHENO
### When running separate sex fOR rv PLEASE REMOVE SEX FROM THE SCRIPT
### burden cmcWald
rvtest --inVcf ~/runs/leahc/UKBB/UKBB_G6PD/UKBB_G6PD_functional_final.vcf.gz --burden cmcWald --kernel skato --geneFile ~/runs/leahc/UKBB/UKBB_G6PD/G6PD.txt  --covar ~/runs/leahc/UKBB/UKBB_G6PD/covar_WITHproxy_UKBB_females.txt --covar-name Age --out UKBB_WITHproxy_G6PD_functional_females --numThread 4 --freqUpper 0.01 --pheno ~/runs/leahc/UKBB/UKBB_G6PD/covar_WITHproxy_UKBB_females.txt --pheno-name PHENO
